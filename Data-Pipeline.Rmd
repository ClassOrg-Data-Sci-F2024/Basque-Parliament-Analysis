
---
title: "Progress-Report-1"
author: "Claire McLean"
date: "2024-10-28"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
NOTE: The dataset I am using for this project is named basque_parliament_1 and was found on https://huggingface.co. The original project was funded by the Spanish Ministry of Science and Innovation, and is licensed as cc0-1.0.
Citation:
@misc {software_technologies_working_group_2024,
author       = { {Software Technologies Working Group} },
title        = { basque_parliament_1 (Revision a2fbcaf) },
year         = 2024,
url          = { https://huggingface.co/datasets/gttsehu/basque_parliament_1 },
doi          = { 10.57967/hf/2485 },
publisher    = { Hugging Face }
}
# Progress Report 1
## Here, I will begin the next steps of my data cleaning.
## Step 1: Performed in Python
In this step, I wrote a script to filter the train.tsv file provided in the Hugging Face dataset folder to include only the rows/utterances that contain the following keywords:
['hablante', 'euskera', 'lenguaje', 'idioma', 'aprendizaje', 'lingüístic']
These keywords were chosen because they demonstrate the relevance of the utterance to the concept of language, linguistics, or "speaker".
The final output of the script was a smaller .csv file containing only the rows that will be relevant to my analysis. This dataframe will be further cleaned in later steps.
This script has been included in my Progress Report 1, but will most likely not be part of my final submission.

## Step 2: Data Wrangling in R

First, import dependencies:
```{r}
library("tidyverse")
library("readr")
library("tidytext")
library("dplyr")
```

## The following steps are the pipeline I followed to split the data into two smaller dataframes. This pipeline can be ignored.

Next, read in the .tsv file:
```{r}
#raw_data <- read_tsv("train.tsv")
#raw_data
```

Finding number of total rows in the raw data:
```{r}
#row_count <- nrow(raw_data)
#row_count
```

Now, breaking up the dataset into two separate parts so that it is possible to push to GitHub (100MB maximum):
```{r}
#row_split1 <- (nrow(raw_data)/2) %>% round(digits = 0)
#row_split2 <- row_split1+1
#raw_data1 <- raw_data %>%
  #slice(1:row_split)
#raw_data2 <- raw_data %>%
  #slice(row_split2:row_count)
```

Exporting the two above datasets into .csv files:
```{r}
#write_csv(raw_data1, "raw_data1.csv")
#write_csv(raw_data2, "raw_data2.csv")
```

## This is where I uploaded the two smaller .csv files:
```{r}
raw_data1 <- read_csv("raw_data1.csv")
raw_data2 <- read_csv("raw_data2.csv")
```

Combining the two dataframes using rbind():
```{r}
raw_data <- rbind(raw_data1, raw_data2)
```

Viewing new, combined dataframe:
```{r}
view(raw_data)
nrow(raw_data)
ncol(raw_data)
```
Starting with 749,945 rows/utterances

Now, filtering to only include rows that are Spanish utterances (exclude Basque and bilingual):
```{r}
small_data_span <- raw_data %>% filter(language == "es")
nrow(small_data_span)
view(small_data_span)
```
Now 524,942 rows/utterances (lost 225,021)

Using stringr to identify utterances/sentences that contain the following keywords. ['hablante', 'euskera', 'lenguaje', 'idioma', 'aprendizaje', 'lingüístic']. This will allow me to perform sentiment analysis only on the utterances that are related in some way to language. Specifically, using str_detect() will allow me to identify occurrences of both 'hablante/es', 'lenguaje/es', 'lingüística/o/as/os'. Further, I created another vector that is used to detect occurrences of ['castellan', 'galleg'], which detects both 'castellano/a' and 'gallego/a'.
```{r}
keywords_vec <- ('hablante|euskera|lenguaje|idioma|lingüístic')
keywords_vec1 <- ('castellan|galleg')
keywords_col <- small_data_span$sentence %>%
  str_detect(keywords_vec)

keywords_col2 <- small_data_span$sentence %>% 
  str_detect(keywords_vec1)

small_data_span2 <- small_data_span %>%
  mutate(keywords = keywords_col) %>% 
  mutate(non_keywords = keywords_col2)

small_data_span3 <- small_data_span2 %>%
  filter(keywords == TRUE) %>% 
  filter(non_keywords == FALSE)
view(small_data_span3)
nrow(small_data_span3)
```
Now 3209 rows (lost 521,715 rows)


Changing name of dataframe:
```{r}
sampled_data <- small_data_span3
```

Now exporting dataframe to .csv file:
```{r}
write_csv(sampled_data, "sampled_data.csv")
```

## Some basic stats:
```{r}
#number of rows:
nrow(sampled_data)
#number of columns:
ncol(sampled_data)
#average utterance length
mean(sampled_data$length)
#number of unique speakers
n_distinct(sampled_data$speaker_id)
```

## Importing SEL lexicon

Load dependencies:
```{r}
library("readxl")
library("readr")
```

Reading in Spanish Emotion Lexicon file:
```{r}
sel <- read_excel("SEL.xlsx")
view(sel)
nrow(sel)
```
Starting with 2,036 emotion words in the SEL.

Viewed final plots and assessed which words carried less emotional 'weight'. Removed these words from the dataframe as they do not contribute much to the analysis. Additionally, created word stemmer to isolate emotion words without gender suffix, as all gendered words in the dataset are masculine:
```{r}
install.packages("SnowballC")
library("SnowballC")

low_ew_vec <- ('cuenta|solo|llevar|llegar')
low_ew <- sel$Palabra %>% 
  str_detect(low_ew_vec)
low_ew2 <- sel %>% 
  mutate('LowEmotion' = low_ew)
sel2 <- sel %>% 
  filter(low_ew == FALSE)

stemmed_sel <- wordStem(sel2$Palabra, language = "spanish")
stemmed_sel <- sel2 %>% 
  mutate(Stems = stemmed_sel) %>% select('#', Stems, PFA, Categoría, -Palabra)
stemmed_sel2 <- stemmed_sel %>% 
  group_by(Categoría) %>% 
  distinct(Stems) %>% 
  ungroup()
nrow(stemmed_sel2)
```
Some of the stems above were originally duplicate stems, with multiple of the same stem occurring in the same emotion. I wanted to keep duplicate stems that were duplicated across different emotions, but delete the ones that were part of the same emotion. In this process, I lost 457 stems.

Importing xlsx file:
```{r}
happiness <- stemmed_sel2 %>% 
  filter(Categoría=='Alegría') %>% 
  nrow()
anger <- stemmed_sel2 %>% 
  filter(Categoría=='Enojo') %>% 
  nrow()
fear <- stemmed_sel2 %>% 
  filter(Categoría=='Miedo') %>% 
  nrow()
disgust <- stemmed_sel2 %>% 
  filter(Categoría=='Repulsión') %>% 
  nrow()
surprise <- stemmed_sel2 %>% 
  filter(Categoría=='Sorpresa') %>% 
  nrow()
sadness <- stemmed_sel2 %>% 
  filter(Categoría=='Tristeza') %>% 
  nrow()
happiness
anger
fear
disgust
surprise
sadness

sel_distribution <- stemmed_sel2 %>%
  group_by(Categoría) %>% 
  mutate(Freq=n()) %>%
  select(Categoría, Freq) %>% 
  distinct()
sel_distribution
```

Tokenizing sentences (ngrams):
```{r}
library(dplyr)
library(tidytext)

sampled_data_ngrams <- sampled_data %>%
  unnest_tokens(tokens, sentence, token="words", drop=FALSE, to_lower=TRUE)
view(sampled_data_ngrams)
```

Stemming sentence tokens (ngrams):
```{r}
token_stems <- wordStem(sampled_data_ngrams$tokens, language = "spanish")
stemmed_ngrams <- sampled_data_ngrams %>% 
  mutate(Stems = token_stems)
stemmed_ngrams
```


The following two code chunks were inspired by a project by Kayleah Griffen, which can be found at the following site: https://rpubs.com/klgriffen96/data607_hw10

Does word correspond to a particular emotion?
```{r}
merged_data <- stemmed_ngrams %>% 
  inner_join(stemmed_sel2, by="Stems", multiple = "all", unmatched = "drop", relationship = "many-to-many") %>% 
  group_by(sentence, speaker_id)

view(merged_data)
```

Grouping by sentence/utterance:
```{r}
merged_data2 <- merged_data %>% 
  arrange(speaker_id, group_by=TRUE)
merged_data2
```

Creating wordclouds for each sentiment:
```{r}
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)

#Ungrouping to create wordclouds
merged_data3 <- merged_data %>% 
  ungroup()
merged_data3
```
Below, I am grouping each emotion and its corresponding tokens into an object. In addition to grouping them, I also deleted duplicate stems that would otherwise present problems in my analysis and kept only the columns that prove to be necessary in plotting the data. I want to avoid certain tokens being plotted multiple times if they are part of the same word in the same utterance.
```{r}
anger_words <- merged_data3 %>% 
  filter(Categoría=="Enojo") %>% 
  group_by(Stems) %>% 
  mutate(Freq = n()) %>% 
  ungroup() %>% 
  select(Stems, Freq, Categoría) %>% 
  distinct(Stems, .keep_all = TRUE)

happiness_words <- merged_data3 %>% 
  filter(Categoría=="Alegría") %>% 
  group_by(Stems) %>% 
  mutate(Freq = n()) %>% 
  ungroup() %>% 
  select(Stems, Freq, Categoría) %>% 
  distinct(Stems, .keep_all = TRUE)

sadness_words <- merged_data3 %>% 
  filter(Categoría=="Tristeza") %>% 
  group_by(Stems) %>% 
  mutate(Freq = n()) %>% 
  ungroup() %>% 
  select(Stems, Freq, Categoría) %>% 
  distinct(Stems, .keep_all = TRUE)

surprise_words <- merged_data3 %>% 
  filter(Categoría=="Sorpresa") %>% 
  group_by(Stems) %>% 
  mutate(Freq = n()) %>% 
  ungroup() %>% 
  select(Stems, Freq, Categoría) %>% 
  distinct(Stems, .keep_all = TRUE)

disgust_words <- merged_data3 %>% 
  filter(Categoría=="Repulsión") %>% 
  group_by(Stems) %>% 
  mutate(Freq = n()) %>% 
  ungroup() %>% 
  select(Stems, Freq, Categoría) %>% 
  distinct(Stems, .keep_all = TRUE)

fear_words <- merged_data3 %>% 
  filter(Categoría=="Miedo") %>% 
  group_by(Stems) %>% 
  mutate(Freq = n()) %>% 
  ungroup() %>% 
  select(Stems, Freq, Categoría) %>% 
  distinct(Stems, .keep_all = TRUE)
```

Generating wordclouds (1) for each sentiment:
```{r}
#anger
set.seed(10)
wordcloud(words = anger_words$tokens, freq = anger_words$Freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.5, scale=c(3.5, 0.3),
          colors=brewer.pal(8, "Set1"))

#happiness
set.seed(10)
wordcloud(words = happiness_words$tokens, freq = happiness_words$Freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.5, scale=c(2.5, 0.3),
          colors=brewer.pal(8, "Set1"))

#sadness
set.seed(10)
wordcloud(words = sadness_words$tokens, freq = sadness_words$Freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.5, scale=c(3.5, 0.3),
          colors=brewer.pal(8, "Set1"))

#surprise
set.seed(10)
wordcloud(words = surprise_words$tokens, freq = surprise_words$Freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.5, scale=c(3, 0.3),
          colors=brewer.pal(8, "Set1"))

#disgust
set.seed(10)
wordcloud(words = disgust_words$tokens, freq = disgust_words$Freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.5, scale=c(3.5, 0.3),
          colors=brewer.pal(8, "Set1"))

#fear
set.seed(8)
wordcloud(words = fear_words$tokens, freq = fear_words$Freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.5, scale=c(3.5, 0.3),
          colors=brewer.pal(8, "Set1"))
```

Generating bar charts for each sentiment:
```{r}
library(ggplot2)

anger_plot <- ggplot(anger_words, aes(tokens, Freq)) + 
  geom_bar(stat = 'identity', fill="red3", width = 0.5) +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = 'Anger Word Frequency')
anger_plot

happiness_plot1 <- ggplot(happiness_words, aes(tokens, Freq)) + 
  geom_bar(stat = 'identity', fill="gold1", width = 0.5) +
  scale_x_discrete(guide = guide_axis(n.dodge=10)) +
  labs(title = 'Happiness Word Frequency')
happiness_plot1

happiness_words2 <- happiness_words %>% 
  filter(Freq > 5)
happiness_plot2 <- ggplot(happiness_words2, aes(tokens, Freq)) + 
  geom_bar(stat = 'identity', fill="gold1", width = 0.5) +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = 'Happiness Word Frequency (n>5)')
happiness_plot2
  

sadness_plot <- ggplot(sadness_words, aes(tokens, Freq)) + 
  geom_bar(stat = 'identity', fill="blue4", width = 0.5) +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = 'Sadness Word Frequency')
sadness_plot

surprise_plot <- ggplot(surprise_words, aes(tokens, Freq)) + 
  geom_bar(stat = 'identity', fill="mediumspringgreen", width = 0.5) +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = 'Surprise Word Frequency')
surprise_plot

disgust_plot <- ggplot(disgust_words, aes(tokens, Freq)) + 
  geom_bar(stat = 'identity', fill="olivedrab1", width = 0.5) +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = 'Disgust Word Frequency')
disgust_plot

fear_plot <- ggplot(fear_words, aes(tokens, Freq)) + 
  geom_bar(stat = 'identity', fill="darkorange", width = 0.5) +
  scale_x_discrete(guide = guide_axis(n.dodge=3)) +
  labs(title = 'Fear Word Frequency')
fear_plot
```

Finding general sentiment for each speaker:
```{r}
by_speaker <- merged_data3 %>% 
  count(speaker_id, Categoría, sort=TRUE) %>%
  filter(n == max(n), .by=speaker_id)
by_speaker

by_speaker %>% distinct(speaker_id) %>% nrow()
```

Plotting composition of general sentiments across speakers:
```{r}
#Out of 122 general sentiments per speaker, find distribution
#Each square represents each speaker (97 speakers total, some speakers have more than one square)
#Picture plot as a parliament session, each member represented by one square
install.packages("waffle")
library(waffle)

comp_waffle <- by_speaker %>%
  group_by(Categoría) %>% 
  mutate(Freq=n()) %>%
  select(Categoría, Freq) %>% 
  distinct()
comp_waffle

waffle(comp_waffle)
```

```{r}
qqnorm(comp_waffle$Freq)
qqline(comp_waffle$Freq)

qqnorm(sel_distribution$Freq)
qqline(sel_distribution$Freq)

scores_words <- (comp_waffle$Freq-(mean(comp_waffle$Freq)))/sd(comp_waffle$Freq)
scores_words
comp_waffle$Freq

scores_usage <- (sel_distribution$Freq-(mean(sel_distribution$Freq)))/sd(sel_distribution$Freq)
scores_usage
sel_distribution$Freq

ggplot(data = sel_distribution, aes(fill=sel_distribution$Categoría, values=Freq)) +
  geom_waffle(color = "white") +
  scale_fill_manual(values = c(Alegría = "seagreen4", Enojo = "slateblue1", 
                               Miedo = "hotpink", Repulsión = "olivedrab3", 
                               Sorpresa = "gold", Tristeza = "tomato"))

prop_words <- data.frame(Category = c('Happiness', 'Sadness', 
                                      'Anger', 'Fear', 'Disgust', 
                                      'Surprise'),
                         Proportion = c((77/122), (21/122), (12/122), (4/122),
                                        (3/122), (5/122)))
prop_words

prop_usage <- data.frame(Category = c('Happiness', 'Sadness', 
                                      'Anger', 'Fear', 'Disgust', 
                                      'Surprise'),
                         Proportion = c((668/2036), (391/2036), (382/2036), (211/2036),
                                        (209/2036), (175/2036)))
prop_usage
```



## Analysis Ideas:
- finding general sentiment for each speaker
- wordcloud of tokens for each sentiment represented in data
  - bar graphs for token and count
  - pander to put them together
- finding general sentiment distribution overall
- vector semantics
- cosine as similarity metric

-make table for sel distribution per emotion
-make wordclouds into html widgets

*Figure out what to do with PFAs and tokens that have more than one emotion

