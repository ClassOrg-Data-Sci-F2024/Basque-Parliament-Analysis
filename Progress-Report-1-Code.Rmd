
---
title: "Progress-Report-1"
author: "Claire McLean"
date: "2024-10-28"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
NOTE: The dataset I am using for this project is named basque_parliament_1 and was found on https://huggingface.co. The original project was funded by the Spanish Ministry of Science and Innovation, and is licensed as cc0-1.0.
Citation:
@misc {software_technologies_working_group_2024,
author       = { {Software Technologies Working Group} },
title        = { basque_parliament_1 (Revision a2fbcaf) },
year         = 2024,
url          = { https://huggingface.co/datasets/gttsehu/basque_parliament_1 },
doi          = { 10.57967/hf/2485 },
publisher    = { Hugging Face }
}
# Progress Report 1
## Here, I will begin the next steps of my data cleaning.
## Step 1: Performed in Python
In this step, I wrote a script to filter the train.tsv file provided in the Hugging Face dataset folder to include only the rows/utterances that contain the following keywords:
['hablante', 'euskera', 'lenguaje', 'idioma', 'aprendizaje', 'lingüístic']
These keywords were chosen because they demonstrate the relevance of the utterance to the concept of language, linguistics, or "speaker".
The final output of the script was a smaller .csv file containing only the rows that will be relevant to my analysis. This dataframe will be further cleaned in later steps.
This script has been included in my Progress Report 1, but will most likely not be part of my final submission.

## Step 2: Data Wrangling in R

First, import dependencies:
```{r}
library("tidyverse")
library("readr")
library("tidytext")
library("dplyr")
```

## The following steps are the pipeline I followed to split the data into two smaller dataframes. This pipeline can be ignored.

Next, read in the .tsv file:
```{r}
#raw_data <- read_tsv("train.tsv")
#raw_data
```

Finding number of total rows in the raw data:
```{r}
#row_count <- nrow(raw_data)
#row_count
```

Now, breaking up the dataset into two separate parts so that it is possible to push to GitHub (100MB maximum):
```{r}
#row_split1 <- (nrow(raw_data)/2) %>% round(digits = 0)
#row_split2 <- row_split1+1
#raw_data1 <- raw_data %>%
  #slice(1:row_split)
#raw_data2 <- raw_data %>%
  #slice(row_split2:row_count)
```

Exporting the two above datasets into .csv files:
```{r}
#write_csv(raw_data1, "raw_data1.csv")
#write_csv(raw_data2, "raw_data2.csv")
```

## This is where I uploaded the two smaller .csv files:
```{r}
raw_data1 <- read_csv("raw_data1.csv")
raw_data2 <- read_csv("raw_data2.csv")
```

Combining the two dataframes using rbind():
```{r}
raw_data <- rbind(raw_data1, raw_data2)
```

Viewing new, combined dataframe:
```{r}
view(raw_data)
nrow(raw_data)
ncol(raw_data)
```


Now, filtering to only include rows that are Spanish utterances (exclude Basque and bilingual):
```{r}
small_data_span <- raw_data %>% filter(language == "es")
view(small_data_span)
```

Now removing the 'path' column as it is unnecessary for my analysis:
```{r}
small_data_span2 <- small_data_span %>% select(-path)
view(small_data_span2)
```

Using stringr to identify utterances/sentences that contain the following keywords. ['hablante', 'euskera', 'lenguaje', 'idioma', 'aprendizaje', 'lingüístic']. This will allow me to perform sentiment analysis only on the utterances that are related in some way to language.
```{r}
keywords_vec <- ('hablante|euskera|lenguaje|idioma|aprendizaje|lingüístic')
keywords_col <- small_data_span2$sentence %>%
  str_detect(keywords_vec)
small_data_span3 <- small_data_span2 %>%
  mutate(keywords = keywords_col)
view(small_data_span3)
small_data_span4 <- small_data_span3 %>%
  filter(keywords == TRUE)
view(small_data_span4)
```

Reducing size by only including 300 random rows:
```{r}
set.seed(10)
sampled_data <- small_data_span4 %>% slice_sample(n=300)
view(sampled_data)
```

Now exporting dataframe to .csv file (annotations still needed):
```{r}
write_csv(sampled_data, "sampled_data.csv")
```

## Some basic stats:
```{r}
#number of rows:
nrow(sampled_data)
#number of columns:
ncol(sampled_data)
#average utterance length
mean(sampled_data$length)
#number of unique speakers
n_distinct(sampled_data$speaker_id)
```

## Importing SEL lexicon

Load dependencies:
```{r}
library("readxl")
library("readr")
```

Importing xlsx file:
```{r}
sel <- read_excel("SEL.xlsx")
```

Unnesting sentence tokens (ngrams):
```{r}
library(dplyr)
library(tidytext)

sampled_data_ngrams <- sampled_data %>%
  unnest_tokens(tokens, sentence, token="words", drop=FALSE, to_lower=TRUE)
sampled_data_ngrams
```
